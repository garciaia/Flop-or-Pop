{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67144f48-d04b-4187-a673-eeed5627f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c782cbf-2819-4c67-86a6-59d9652ea906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains \"things\" that are common to both\n",
    "# versions. \n",
    "\n",
    "# Function for Grouping\n",
    "# I didn't really think about how it should be grouped,\n",
    "# I was just trying to create as many classes as possible.\n",
    "# So, treat these groups as a mere example.\n",
    "def group(streams):\n",
    "    # Each group has a lower and upper bound.\n",
    "    groups = [(0, 1000000, \"Group A\"), (1000000, 5000000, \"Group B\"), (5000000, 10000000, \"Group C\"), (10000000, 100000000, \"Group D\"),]\n",
    "    for l, r, groupName in groups:\n",
    "        if l <= streams <= r:\n",
    "            return groupName\n",
    "    return \"Group E\"\n",
    "\n",
    "# Non-Numerical Features; do we need track??\n",
    "dropFeatures = ['Unnamed: 0', 'Artist', 'Url_spotify', 'Track', 'Album', 'Album_type', 'Uri', 'Title', 'Channel', 'Views', 'Likes', 'Comments', 'Description', 'Licensed', 'official_video', 'Url_youtube']\n",
    "\n",
    "# Data\n",
    "data = pd.read_csv(\"data/Spotify_Youtube.csv\")\n",
    "\n",
    "# for column in categorical_columns:\n",
    "#     data[column] = LabelEncoder().fit_transform(data[column])\n",
    "\n",
    "data.drop(dropFeatures, axis=1, inplace=True)\n",
    "\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1069a68b-7da5-4fa4-be85-edff2d50480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification (Grouping Done Before)\n",
    "# I think Jaden proposed that we partition the streams beforehand and use classification.\n",
    "# So, here is something like that.\n",
    "\n",
    "# x = data.drop(\"Stream\", axis=1) \n",
    "# x_scaler = StandardScaler()\n",
    "# x_scaled = x_scaler.fit_transform(x)\n",
    "\n",
    "# y = data[\"Stream\"].values.reshape(-1, 1)\n",
    "# y_scaler = StandardScaler()\n",
    "# y_scaled = y_scaler.fit_transform(y)\n",
    "\n",
    "x = data.drop(columns=[\"Stream\"])\n",
    "y = data[\"Stream\"]\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x_scaled, y, test_size=0.2)\n",
    "\n",
    "# # Model\n",
    "# # Using the default final estimator (LogisticRegression)\n",
    "# st = StackingClassifier(estimators=[\n",
    "#     ('LogisticRegression', LogisticRegression()), \n",
    "#     ('KNeighbors', KNeighborsClassifier()), \n",
    "#     ('DecisionTree', DecisionTreeClassifier())\n",
    "# ])\n",
    "\n",
    "# # Fit and Predict\n",
    "# st.fit(xTrain, yTrain)\n",
    "# yPred = st.predict(xTest)\n",
    "\n",
    "# # This has a score of 1.0,\n",
    "# # I'm not sure if that means anything\n",
    "# st.score(xTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "188dfa1e-d13f-437b-b2f0-3486eb646cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression (Grouping Done After)\n",
    "# Ana proposed that we group the streams after the prediction;\n",
    "# so we'd use regression to predict the number of streams, and then we'd\n",
    "# return the group said stream belongs to (i.e. streams < 100,000,000 belong to \"flop\").\n",
    "\n",
    "# Extracting\n",
    "x = StandardScaler().fit_transform(data)\n",
    "y = data[\"Stream\"]\n",
    "y_scaled = StandardScaler().fit_transform(y.values.reshape(-1, 1))\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8389d442-2222-4ca3-bfac-98c6259b2d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(y_true, predictions, model_name):\n",
    "    mse = mean_squared_error(yTest, predictions)\n",
    "    mae = mean_absolute_error(yTest, predictions)\n",
    "    r2 = r2_score(yTest, predictions)\n",
    "\n",
    "    print(f\"{model_name} Evaluation Metrics:\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be5a9e4-e447-4760-a638-a1e756095d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Kernel Evaluation Metrics:\n",
      "Mean Squared Error: 1.178916833955166e-13\n",
      "Mean Absolute Error: 2.724974015736888e-07\n",
      "R-squared: 1.0\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "model.fit(xTrain, yTrain)\n",
    "\n",
    "linear_regr_pred = model.predict(xTest)\n",
    "evaluate_model(yTest,linear_regr_pred,\"Linear Regression Kernel\")\n",
    "\n",
    "# bg_pred = bg.predict(xTest)\n",
    "# evaluate_model(yTest,bg_pred,\"BG Model\")\n",
    "\n",
    "# print(\"Max Stream Value:\", y.max())\n",
    "# print(\"Min Stream Value:\", y.min())\n",
    "# print(\"Mean Stream Value:\", y.mean())\n",
    "\n",
    "# st_pred = st.predict(xTest)\n",
    "# evaluate_model(yTest,st_pred,\"St Kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2b9b412-3917-4fa6-ac99-8e737154c894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3900539972.5257874"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "# I'm just using the default estimator provided for the final estimator (RidgeCV).\n",
    "st = StackingRegressor(estimators=[\n",
    "    ('KNeighbors', KNeighborsRegressor()), \n",
    "    ('LinearRegression', LinearRegression()), \n",
    "    ('DecisionTree', DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "# Fitting and Scoring\n",
    "st.fit(xTrain, yTrain)\n",
    "\n",
    "# This has a dire score,\n",
    "# I think it's caused by the models being used?\n",
    "# Maybe stacking is not the best way to go here.\n",
    "st.score(xTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f1038a-aa12-4e78-944d-f6f21057baa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BG Model Evaluation Metrics:\n",
      "Mean Squared Error: 1.9263984209087927e-14\n",
      "Mean Absolute Error: 9.366606943575894e-08\n",
      "R-squared: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Regression (Continued)\n",
    "# When bagging is used, the R^2 score is closer to 1.\n",
    "# As a side note, it had a score of 0.9999371139478829.\n",
    "# So, if we do regression, maybe we use bagging.\n",
    "# Also, I'm using the default model for BaggingRegressor,\n",
    "# which I think is a Decision Tree?\n",
    "bg = BaggingRegressor(estimator=LinearRegression())\n",
    "bg.fit(xTrain, yTrain)\n",
    "\n",
    "bg_pred = bg.predict(xTest)\n",
    "evaluate_model(yTest,bg_pred,\"BG Model\")\n",
    "\n",
    "# dt = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "# dt.fit(xTrain, yTrain)\n",
    "# dt_predictions = dt.predict(xTest)\n",
    "# evaluate_model(yTest,dt_predictions,\"Decision Tree Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2df18dd-1c38-48ae-bd16-49763104c290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_scaled shape: (20140, 11)\n",
      "xTest shape: (4028, 12)\n",
      "Training columns: Index(['Danceability', 'Energy', 'Key', 'Loudness', 'Speechiness',\n",
      "       'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo',\n",
      "       'Duration_ms'],\n",
      "      dtype='object')\n",
      "Test columns: No column names\n"
     ]
    }
   ],
   "source": [
    "print(\"x_scaled shape:\", x_scaled.shape)\n",
    "print(\"xTest shape:\", xTest.shape)\n",
    "\n",
    "print(\"Training columns:\", data.drop(\"Stream\", axis=1).columns)\n",
    "print(\"Test columns:\", xTest.columns if hasattr(xTest, 'columns') else \"No column names\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0259bcec-f185-49e5-a193-fe1c68312353",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 12 features, but RandomForestRegressor is expecting 11 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m rf\u001b[38;5;241m.\u001b[39mfit(x_scaled, y)\n\u001b[1;32m----> 5\u001b[0m rf_pred \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mpredict(xTest)\n\u001b[0;32m      6\u001b[0m evaluate_model(yTest,rf_pred,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandomForestRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CIS4930\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:1063\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1061\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m-> 1063\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X)\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CIS4930\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:641\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    639\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 641\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    642\u001b[0m     X,\n\u001b[0;32m    643\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE,\n\u001b[0;32m    644\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    645\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    646\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m    647\u001b[0m )\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CIS4930\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\CIS4930\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 12 features, but RandomForestRegressor is expecting 11 features as input."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(x_scaled, y)\n",
    "\n",
    "rf_pred = rf.predict(xTest)\n",
    "evaluate_model(yTest,rf_pred,\"RandomForestRegressor\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
